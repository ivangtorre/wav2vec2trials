program: main/train_wav2vec2_sweep.py
project: wav2vec2aphasia
method: random
metric:
  name: test/wer
  goal: minimize
parameters:
  activation_dropout:
    distribution: uniform
    min: 0.01
    max: 0.04
  attention_dropout:
    distribution: uniform
    min: 0.01
    max: 0.1
  hidden_dropout:
    distribution: uniform
    min: 0.01
    max: 0.05
  feat_proj_dropout:
    distribution: uniform
    min: 0.01
    max: 0.1
  mask_time_prob:
    distribution: uniform
    min: 0.03
    max: 0.15
  layerdrop:
    distribution: uniform
    min: 0.01
    max: 0.1
  learning_rate:
    distribution: uniform
    min: 0.00005
    max: 0.0005
  gradient_accumulation_steps:
    values:
    - 2
    - 3
    - 4
command:
  - python3
  - "-m"
  - "torch.distributed.launch"
  - "--nproc_per_node"
  - 4
  - "--nnodes"
  - 1
  - "--node_rank"
  - 0
  - ${program}
  - "--model_name_or_path"
  - "facebook/wav2vec2-large-xlsr-53"
  - "--dataset_config_name"
  - "df_final.csv"
  - "--output_dir"
  - "results"
  - "--overwrite_output_dir"
  - "--num_train_epochs"
  - 10
  - "--per_device_train_batch_size"
  - 2
  - "--warmup_ratio"
  - 0.1
  - "--lr_scheduler_type"
  - "linear"
  - "--evaluation_strategy"
  - "steps"
  - "--save_steps"
  - 3000
  - "--eval_steps"
  - 3000
  - "--logging_steps"
  - 100
  - "--save_total_limit"
  - 2
  - "--freeze_feature_extractor"
  - "--fp16"
  - "--do_train"
  - "--do_eval"
  - "--cache_dir"
  - "/home/VICOMTECH/igonzalez/APHASIA/audiosV3/"
  - "--model_cache_dir"
  - "/datasets/modelxlsr"
  - "--logging_dir"
  - "/results"
  - "--preprocessing_num_workers"
  - 2
  - "--max_train_samples"
  - 0
  - "--max_val_samples"
  - 0
  - ${args}
