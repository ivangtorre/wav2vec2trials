program: main/train_wav2vec2_sweep.py
project: wav2vec2aphasia
method: random
metric:
  name: test/wer
  goal: minimize
parameters:
  activation_dropout:
    values:
    - 0.026
  attention_dropout:
    values:
    - 0.036
  hidden_dropout:
    values:
    - 0.0296
  feat_proj_dropout:
    values:
    - 0.0296
  mask_time_prob:
    values:
    - 0.057
  layerdrop:
    values:
    - 0.014
  learning_rate:
    values:
    - 0.00005
    - 0.0001
    - 0.0002
  gradient_accumulation_steps:
    values:
    - 3
  num_train_epochs:
    values:
    - 10
    - 20
    - 50
command:
  - python3
  - "-m"
  - "torch.distributed.launch"
  - "--nproc_per_node"
  - 1
  - "--nnodes"
  - 1
  - "--node_rank"
  - 0
  - ${program}
  - "--model_name_or_path"
  - "facebook/wav2vec2-large-xlsr-53"
  - "--dataset_config_name"
  - "df_final.csv"
  - "--output_dir"
  - "results"
  - "--overwrite_output_dir"
  - "--per_device_train_batch_size"
  - 20
  - "--warmup_ratio"
  - 0.1
  - "--lr_scheduler_type"
  - "linear"
  - "--evaluation_strategy"
  - "steps"
  - "--save_steps"
  - 3000
  - "--eval_steps"
  - 3000
  - "--logging_steps"
  - 100
  - "--save_total_limit"
  - 2
  - "--freeze_feature_extractor"
  - "--fp16"
  - "--do_train"
  - "--do_eval"
  - "--cache_dir"
  - "/home/VICOMTECH/igonzalez/APHASIA/audiosV3/"
  - "--model_cache_dir"
  - "/datasets/modelxlsr"
  - "--logging_dir"
  - "/results"
  - "--preprocessing_num_workers"
  - 4
  - "--max_train_samples"
  - 0
  - "--max_val_samples"
  - 0
  - ${args}
